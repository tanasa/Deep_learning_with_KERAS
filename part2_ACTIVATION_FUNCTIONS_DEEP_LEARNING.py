# -*- coding: utf-8 -*-
"""HOME_WORK_2_DEEP_LEARNING

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tjmS_ZwWQa9os1DiQe7LQWdAb6vsw7Gm
"""

import numpy as np
scores = np.array([2.0, 1.0, 0.1])

def softmax_numpy(scores) : 
    return np.exp(scores)/np.sum(np.exp(scores), axis =0)

prob = softmax_numpy(scores)
print(prob)

import tensorflow as tf
print(tf.__version__)

scores = tf.constant([2.0, 1.0, 0.1])
prob2 = tf.nn.softmax(scores)
print(prob2)

# ARGMAX

import numpy as np
a = np.array([(8, 2, 7), (3,4,5)])

print("Sum of Columns")
sumColumns = a.sum(axis = 0)
print(sumColumns)

print("Sum of Rows")
sumRows = a.sum(axis = 1)
print(sumRows)



import numpy as np
a = np.array([(8, 2, 7), (3,4,5)])

print("Index of maximum column")
indexColumns = a.argmax(axis = 0)
print(indexColumns)

print("Index of maximum row")
indexRows = a.argmax(axis = 1)
print(indexRows)

print("Sum of Columns")
sumColumns = a.sum(axis = 0)
print(sumColumns)

print("Sum of Rows")
sumRows = a.sum(axis = 1)
print(sumRows)





inputData = tf.constant([[10,20]])
print(inputData.shape)
print(inputData)

#################### layer 1

W1 = tf.constant([[1,2,3], [4,5,6]])
print(W1.shape)
print(W1)

##############################

print("Index of maximum row")
indexRows = tf.argmax(a, axis = 1)
print(indexRows)

inputData = tf.constant([[10,20]])
print(inputData.shape)
print(inputData)

#################### layer 1

W1 = tf.constant([[1,2,3], [4,5,6]])
print(W1.shape)
print(W1)

##############################

inputData = tf.constant([[10,20]])
print(inputData.shape)
print(inputData)

#################### layer 1

W1 = tf.constant([[1,2,3], [4,5,6]])
print(W1.shape)
print(W1)

##############################

##############################
##############################

inputData = tf.constant([[10,20]])
print(inputData.shape)
print(inputData)

#################### layer 1
##############################

W1 = tf.constant([[1,2,3], [4,5,6]])
print(W1.shape)
print(W1)

##############################

b1 = tf.constant([[7,8,9]])
print(b1.shape)
print(b1)

##############################

outputH1 = tf.matmul(inputData, W1) + b1
print(outputH1.shape)
print(outputH1)

outputH1_Activation = tf.sigmoid(tf.cast(outputH1, tf.float32))
print(outputH1_Activation.shape)
print(outputH1_Activation)

##############################

W2 = tf.cast(tf.constant([[10], [11], [12], tf.float32)
print(W2.shape)
print(W2)

##############################

b2 = tf.cast(tf.constant([[13]], tf.float32)
print(b2.shape)
print(b2)

##############################

outputH2 = tf.matmul(outputH1_Activation, W2) + b2
print(outputH2.shape)
print(outputH2)

outputH2_Activation = tf.sigmoid(tf.cast(outputH2, tf.float32))
print(outputH2_Activation.shape)
print(outputH2_Activation)

##############################
##############################





import tensorflow as tf

#############################
##############################

inputData = tf.constant([[0.1,0.2]])
# print(inputData.shape)
# print(inputData)

#################### layer 1
##############################

W1 = tf.constant([[0.15], [0.05]])
# print(W1.shape)
# print(W1)

##############################

b1 = tf.constant([[0.33]])
# print(b1.shape)
# print(b1)

##############################

outputH1 = tf.matmul(inputData, W1) + b1
# print(outputH1.shape)
# print(outputH1)

outputH1_Activation = tf.keras.activations.relu(tf.cast(outputH1, tf.float32))
# print(outputH1_Activation.shape)
# print(outputH1_Activation)

##############################

W2 = tf.constant([[0.36]])
# print(W2.shape)
# print(W2)

##############################

b2 = tf.constant([[0.56]])
# print(b2.shape)
# print(b2)

##############################

outputH2 = tf.matmul(outputH1_Activation, W2) + b2
# print(outputH2.shape)
# print(outputH2)

outputH2_Activation = tf.keras.activations.relu(tf.cast(outputH2, tf.float32))
# print(outputH2_Activation.shape)
# print(outputH2_Activation)

##############################
##############################

print(outputH1_Activation)
print(outputH2_Activation)



import tensorflow as tf

x1 = -4.0
x2 = 0.5
x3 = 4.0 

gradient_SIGMOID1 = tf.keras.activations.sigmoid(x1) * (1- tf.keras.activations.sigmoid(x1))
print(gradient_SIGMOID1)

gradient_SIGMOID2 = tf.keras.activations.sigmoid(x2) * (1- tf.keras.activations.sigmoid(x2))
print(gradient_SIGMOID2)

gradient_SIGMOID3 = tf.keras.activations.sigmoid(x3) * (1- tf.keras.activations.sigmoid(x3))
print(gradient_SIGMOID3)

import tensorflow as tf

x1 = -4.0
x2 = 0.5
x3 = 4.0 

gradient_TANH1 = 1 - tf.keras.activations.tanh(x1)**2
print(gradient_TANH1)

gradient_TANH2 = 1 - tf.keras.activations.tanh(x2)**2
print(gradient_TANH2)

gradient_TANH3 = 1- tf.keras.activations.tanh(x3)**2
print(gradient_TANH3)

x1 = -4.0
x2 = 0.5
x3 = 4.0 

gradient_RELU = (lambda x: 1 if ( x > 0) else 0)(x1)
print(gradient_RELU)

gradient_RELU = (lambda x: 1 if ( x > 0) else 0)(x2)
print(gradient_RELU)

gradient_RELU = (lambda x: 1 if ( x > 0) else 0)(x3)
print(gradient_RELU)



## PROBLEM 3
## here working on the PROBLEM 3, according to the SLIDE 22 in the presentation

# the SYNTAX is DIFFERENT than the syntax that was provided in the file with the ANSWERS

import tensorflow as tf
print(tf.__version__)
##############################

SCORES1 =  tf.constant([2.3, 1.2, 0.3, 0.0])
PROBAB1 = tf.nn.softmax(SCORES1)
print(PROBAB1) 

##############################

SCORES2 =  tf.constant([1.9, 1.7, 2.6, 0.2, 1.3])
PROBAB2 = tf.nn.softmax(SCORES2)
print(PROBAB2)







import tensorflow as tf
print(tf.__version__)
##############################

SCORES1 =  tf.constant([2.3, 1.2, 0.3, 0.0])
PROBAB1 = tf.nn.softmax(SCORES1)
print(PROBAB1) 

##############################

SCORES2 =  tf.constant([1.9, 1.7, 2.6, 0.2, 1.3])
PROBAB2 = tf.nn.softmax(SCORES2)
print(PROBAB2)

## PROBLEM 4, regarding the CROSS-ENTROPY, according to the SLIDE 24 of the presentation

## the SYNTAX is DIFFERENT than the syntax that was provided in the file with the ANSWERS

import tensorflow as tf
print(tf.__version__)
import math
import numpy as np

############################## here we define a function that we call CROSS-ENTROPY
def CROSS_ENTROPY (TARGET, COMPUTED) :
    z = -1 * ( TARGET * math.log(COMPUTED) + (1-TARGET) * math.log(1-COMPUTED) )
    return z  
##############################

TARGET1 = tf.constant([0.0, 0.0, 0.0, 0.0, 0.0])
COMPUTED1 = tf.constant([0.95, 0.8, 0.6, 0.4, 0.1])

# using the NATURAL LOGARITHM
print("        ")

for i in range(len(TARGET1)) : 
    # print(CROSS_ENTROPY(TARGET1[i], COMPUTED1[i]))
    print('Computed value is', np.array(COMPUTED1[i]) , 'Cost function is', np.array(CROSS_ENTROPY(TARGET1[i], COMPUTED1[i]))) 

# using a a line to separate
print("        ")

TARGET2 = tf.constant([1.0, 1.0, 1.0, 1.0, 1.0])
COMPUTED2 = tf.constant([0.95, 0.8, 0.6, 0.4, 0.1])

for i in range(len(TARGET2)) : 
    # print(CROSS_ENTROPY(TARGET2[i], COMPUTED2[i]))
    print('Computed value is', np.array(COMPUTED2[i]) , 'Cost function is', np.array(CROSS_ENTROPY(TARGET2[i], COMPUTED2[i])))

# working on the PROBLEM 5
# according to the SLIDE 26

import tensorflow as tf
print(tf.__version__)
import math
import numpy as np


a = tf.constant( [ [5,2,3], [26, 56, 92], [3, 0, 26] ])
print(np.array(x))

print("the index of the maximum column")
a1 = tf.argmax(a, axis = 0 )
print(np.array(a1))

print("the index of the maximum row")
a2 = tf.argmax(a, axis = 1 )
print(np.array(a2))



# working on PROBLEM 6
# we have adapted a previous code a PREVIOUS PROBLEM

# in the first BATCH, the INPUT is [0,0], and the OUTPUT is 0
# in the first BATCH, the INPUT is [1,0], and the OUTPUT is 1
# in the first BATCH, the INPUT is [0,1], and the OUTPUT is 1
# in the first BATCH, the INPUT is [1,1], and the OUTPUT is 0

# shall we use a BATCH SIZE of 1

import tensorflow as tf

############################## the INPUT data is here 0, 0
##############################

inputData = tf.constant([[0, 0]])

W1 = tf.constant([[-4, -6, -5], [3, 6, 4 ]])
b1 = tf.constant([[-2 , 3 , -2]])

outputH1 = tf.matmul(inputData, W1) + b1
outputH1_Activation = tf.keras.activations.sigmoid(tf.cast(outputH1, tf.float32))

##############################

W2 = tf.constant([ [5], [-9], [7] ], tf.float32)
b2 = tf.constant([[4]], tf.float32)

outputH2 = tf.matmul(outputH1_Activation, W2) + b2
outputH2_Activation = tf.keras.activations.sigmoid(tf.cast(outputH2, tf.float32))

##############################
##############################

print(np.array(outputH1_Activation))
print(np.array(outputH2_Activation))

##############################
##############################

# the OUTPUT that we do have is

# the TRUE OUTPUT is 0
# so we do compute the ERROR

TRUE_OUTPUT1 = 0

ERROR1 = (TRUE_OUTPUT1 - outputH2_Activation ) ** 2

print(np.array([ERROR1]))
##############################
##############################

# shall we use a BATCH SIZE of 1

import tensorflow as tf

############################## the INPUT data is here 1, 0
##############################

inputData = tf.constant([[1, 0]])

W1 = tf.constant([[-4, -6, -5], [3, 6, 4 ]])
b1 = tf.constant([[-2 , 3 , -2]])

outputH1 = tf.matmul(inputData, W1) + b1
outputH1_Activation = tf.keras.activations.sigmoid(tf.cast(outputH1, tf.float32))

##############################

W2 = tf.constant([ [5], [-9], [7] ], tf.float32)
b2 = tf.constant([[4]], tf.float32)

outputH2 = tf.matmul(outputH1_Activation, W2) + b2
outputH2_Activation = tf.keras.activations.sigmoid(tf.cast(outputH2, tf.float32))

##############################
##############################

print(np.array(outputH1_Activation))
print(np.array(outputH2_Activation))

##############################
##############################

# the OUTPUT that we do have is

# the TRUE OUTPUT is 1
# so we do compute the ERROR

TRUE_OUTPUT2 = 1

ERROR2 = (TRUE_OUTPUT2 - outputH2_Activation ) ** 2

print(np.array([ERROR2]))
##############################
##############################

# shall we use a BATCH SIZE of 1

import tensorflow as tf

############################## the INPUT data is here 0, 1
##############################

inputData = tf.constant([[0, 1]])

W1 = tf.constant([[-4, -6, -5], [3, 6, 4 ]])
b1 = tf.constant([[-2 , 3 , -2]])

outputH1 = tf.matmul(inputData, W1) + b1
outputH1_Activation = tf.keras.activations.sigmoid(tf.cast(outputH1, tf.float32))

##############################

W2 = tf.constant([ [5], [-9], [7] ], tf.float32)
b2 = tf.constant([[4]], tf.float32)

outputH2 = tf.matmul(outputH1_Activation, W2) + b2
outputH2_Activation = tf.keras.activations.sigmoid(tf.cast(outputH2, tf.float32))

##############################
##############################

print(np.array(outputH1_Activation))
print(np.array(outputH2_Activation))

##############################
##############################

# the OUTPUT that we do have is

# the TRUE OUTPUT is 1
# so we do compute the ERROR

TRUE_OUTPUT3 = 1

ERROR3 = (TRUE_OUTPUT3 - outputH2_Activation ) ** 2

print(np.array([ERROR3]))
##############################
##############################

# shall we use a BATCH SIZE of 1

import tensorflow as tf

############################## the INPUT data is here 1, 1
##############################

inputData = tf.constant([[1, 1]])

W1 = tf.constant([[-4, -6, -5], [3, 6, 4 ]])
b1 = tf.constant([[-2 , 3 , -2]])

outputH1 = tf.matmul(inputData, W1) + b1
outputH1_Activation = tf.keras.activations.sigmoid(tf.cast(outputH1, tf.float32))

##############################

W2 = tf.constant([ [5], [-9], [7] ], tf.float32)
b2 = tf.constant([[4]], tf.float32)

outputH2 = tf.matmul(outputH1_Activation, W2) + b2
outputH2_Activation = tf.keras.activations.sigmoid(tf.cast(outputH2, tf.float32))

##############################
##############################

print(np.array(outputH1_Activation))
print(np.array(outputH2_Activation))

##############################
##############################

# the OUTPUT that we do have is

# the TRUE OUTPUT is 1
# so we do compute the ERROR

TRUE_OUTPUT4 = 0

ERROR4 = (TRUE_OUTPUT4 - outputH2_Activation ) ** 2

print(np.array([ERROR4]))
##############################
##############################

# shall we use a BATCH SIZE of 4

##############################

import tensorflow as tf

############################## the INPUT data is here 1, 1
##############################

inputData = tf.constant([[0,0], [1,0], [0,1], [1, 1]])

W1 = tf.constant([[-4, -6, -5], [3, 6, 4 ]])
b1 = tf.constant([[-2 , 3 , -2]])

outputH1 = tf.matmul(inputData, W1) + b1
outputH1_Activation = tf.keras.activations.sigmoid(tf.cast(outputH1, tf.float32))

##############################

W2 = tf.constant([ [5], [-9], [7] ], tf.float32)
b2 = tf.constant([[4]], tf.float32)

outputH2 = tf.matmul(outputH1_Activation, W2) + b2
outputH2_Activation = tf.keras.activations.sigmoid(tf.cast(outputH2, tf.float32))

##############################
##############################

# print(np.array(outputH1_Activation))
# print(np.array(outputH2_Activation))

##############################
##############################

# the OUTPUT that we do have is

# the TRUE OUTPUT is 0, 1, 1, 0
# so we do compute the ERROR

TRUE_OUTPUT4 = tf.constant( [ [0.0], [1.0], [1.0], [0.0] ] )

ERROR4 = (TRUE_OUTPUT4 - outputH2_Activation ) ** 2

print("here it is the ERROR that we do obtain for each INPUT type")
print(np.array([ERROR4]))
##############################
##############################











